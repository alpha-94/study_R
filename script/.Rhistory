# (2) 단어색상과 글꼴 지정
pal <- brewer.pal(12, "paired")
windowsFonts(malgun=windowsFonts("맑은 고딕"))
# (3) 단어 구름 시각화
x11() # 별도의 창을 띄우는 함수
wordcloud(word.df$word, word.df$freq, scale = c(5,1), min.freq = 3, random.order = F, rot.per = .1, colors = pal, family="malgun")
# (2) 단어색상과 글꼴 지정
pal <- brewer.pal(12, "paired")
# (2) 단어색상과 글꼴 지정
pal <- brewer.pal(12, "paired")
# (2) 단어색상과 글꼴 지정
pal <- brewer.pal(12, "paired")
?brewer.pal
# (2) 단어색상과 글꼴 지정
pal <- brewer.pal(12, "Paired")
windowsFonts(malgun=windowsFonts("맑은 고딕"))
# (3) 단어 구름 시각화
x11() # 별도의 창을 띄우는 함수
wordcloud(word.df$word, word.df$freq, scale = c(5,1), min.freq = 3, random.order = F, rot.per = .1, colors = pal, family="malgun")
# 1) 실습 파일 가져오기
data <- read.csv("C:/workspaces/bigdata-master/R/data/cleanDescriptive.csv", header = T)
View(data)
# 2) 변수 리코딩
x <- data$level2 # 리코딩 변수 이용(학력수준)
y <- data$pass2  # 리코딩 변수 이용(합격/불합격)
x; y  # 부모학력수준(x) -> 자녀대학진학여부(y)
# 3) 데이터프레임 생성
result <- data.frame(Level=x, Pass=y) # 데이터 프레임 생성 - 데이터 묶음.
# 3) 데이터프레임 생성
result <- data.frame(Level=x, Pass=y) # 데이터 프레임 생성 - 데이터 묶음.
library(gmodels)
# 3) 패키지를 이용한 교차 분할표 생성
CrossTable(x, y) # gmodels
chisq.test(c(4,6,17,16,8,9))
# 5개 제품의 스포츠 음료에 대한 선호도에 차이가 있는지 검정
data <- textConnection(
"스포츠음료종류  관측도수
1    41
2    30
3    51
4    71
5    61
")
x <- read.table(data, header = T)
str(x)
chisq.test(x$관측도수)
data <- read.csv("C:/workspaces/R/data/cleanDescriptive.csv", header = T)
# 독립변수(x) = 설명변수, 종속변수(y) = 반응 변수 생성
x <- data$level2 # 부모의 학력수준
y <- data$pass2  # 자녀의 대학 진학 여부
CrossTable(x, y, chisq = T)
data <- read.csv("C:/workspaces/bigdata-master/R/data/cleanDescriptive.csv", header = T)
# 독립변수(x) = 설명변수, 종속변수(y) = 반응 변수 생성
x <- data$level2 # 부모의 학력수준
y <- data$pass2  # 자녀의 대학 진학 여부
CrossTable(x, y, chisq = T)
# 1. 파일 가져오기
data <- read.csv("C:/workspaces/bigdata-master/R/data/homogenity.csv", header = T)
View(data)
# 전처리 - 결측치/ method와 survey 변수만 서브셋 생성
data <- subset(data, !is.na(survey), c(method, survey))
data
data$method2[data$method == 1] <- "방법1"
data$method2[data$method == 2] <- "방법2"
data$method2[data$method == 3] <- "방법3"
# survey2 필드 추가
data$survey2[data$survey == 1] <- "1. 매우만족"
data$survey2[data$survey == 2] <- "2. 만족"
data$survey2[data$survey == 3] <- "3. 보통"
data$survey2[data$survey == 4] <- "4. 불만족"
data$survey2[data$survey == 5] <- "5. 매우불만족"
table(data$method)
table(data$survey)
# 1. 파일 가져오기
data <- read.csv("C:/workspaces/bigdata-master/R/data/homogenity.csv", header = T)
table(data$method) # 교육방법
# 1  2  3
#50 50 50
table(data$survey) # 만족도
# 1  2  3  4  5
#21 29 37 42 21
# 전처리 - 결측치/ method와 survey 변수만 서브셋 생성
data <- subset(data, !is.na(survey), c(method, survey))
is.na(data$survey)
# 1. 파일 가져오기
data <- read.csv("C:/workspaces/bigdata-master/R/data/homogenity.csv", header = T)
is.na(data$survey)
# 1. 파일 가져오기
data <- read.csv("C:/workspaces/bigdata-master/R/data/homogenity.csv", header = T)
View(data)
# 1. 파일 가져오기
data <- read.csv("C:/workspaces/bigdata-master/R/data/homogenity.csv", header = T)
table(data$method) # 교육방법
# 1  2  3
#50 50 50
table(data$survey) # 만족도
# 1  2  3  4  5
#21 29 37 42 21
# 전처리 - 결측치/ method와 survey 변수만 서브셋 생성
data <- subset(data, !is.na(survey), c(method, survey))
# 3. 교차분할표 작성
table(data$method2, data$survey2) # 교차표 생성 -> table(행, 열)
# method2 필드 추가
data$method2[data$method == 1] <- "방법1"
data$method2[data$method == 2] <- "방법2"
data$method2[data$method == 3] <- "방법3"
# survey2 필드 추가
data$survey2[data$survey == 1] <- "1. 매우만족"
data$survey2[data$survey == 2] <- "2. 만족"
data$survey2[data$survey == 3] <- "3. 보통"
data$survey2[data$survey == 4] <- "4. 불만족"
data$survey2[data$survey == 5] <- "5. 매우불만족"
# 3. 교차분할표 작성
table(data$method2, data$survey2) # 교차표 생성 -> table(행, 열)
# 4. 교차분할표 생성
CrossTable(data$method2, data$survey2, chisq = T)
# 모평균 신뢰수준 95%의 신뢰구간(하한값과 상한값) 구하기
N <- 10000  # 표본크기
X <- 165.1  # 표본평균
S <- 2      # 표본 표준편차
low <- X - 1.96 * S / sqrt(N) # 신뢰구간 하한값
high <- X + 1.96 * S / sqrt(N) # 신뢰구간 상한값
low; high  # 165.0608 ~ 165.1392
# 신뢰구간으로 표본오차 구하기
low - X  # -0.0392 = 신뢰구간 하한값 - 표본 평균
high -X  # 0.0392 = 신뢰구간 상한값 - 표본 평균
# 단계1. 실습데이터 가져오기
getwd()
data <- read.csv("C:/workspaces/bigdata-master/R/data/one_sample.csv", header = T)
head(data)
View(data)
# 단계2. 빈도수와 비율 계산
x <- data$survey
x
summary(x) # 결측치 확인
length(x) # [1] 150
table(x) # 빈도수(0:불만족(14), 1:만족(136))
table(x) # 빈도수(0:불만족(14), 1:만족(136))
# 단계3. 패키지 이용 빈도수와 비율계산
library(prettyR) # freq() 함수 사용
# 단계3. 패키지 이용 빈도수와 비율계산
install.packages("prettyR")
library(prettyR) # freq() 함수 사용
freq(x)
# 1) 양측검정
binom.test(c(136, 14), p=0.8) # 기존 80% 만족율 기준 검증 실시
binom.test(c(136, 14), p=0.8, alternative = "two.sided", conf.level = 0.95)
# 이항분포 불만족율 기준 비율검정
# 1) 양측검정
binom.test(c(14, 136), p=0.2) # 기존 20% 불만족율 기준 검증 실시
# 이항분포 불만족율 기준 비율검정
# 1) 양측검정
binom.test(c(14, 136), p=0.2) # 기존 20% 불만족율 기준 검증 실시
# 2) 방향성을 갖는 단측 가설 검정
binom.test(c(136, 14), p=0.8, alternative = "greater", conf.level = 0.95) # p-value = 0.0003179
binom.test(c(136, 14), p=0.8, alternative = "less", conf.level = 0.95) # p-value = 0.9999
binom.test(c(136, 14), p=0.8, alternative = "two.sided", conf.level = 0.95)
# 단계1. 실습 파일 가져오기
data <- read.csv("C:/workspaces/bigdata-master/R/data/one_sample.csv", header = T)
head(data)
str(data)
x <- data$time
head(x)
# 단계2. 데이터 분포/결측치 제거
summary(x) # NA's -> 41
mean(x)    # [1] NA
# 단계3. 데이터 정제
mean(x, na.rm=T) # NA 제외 평균(방법1) - [1] 5.556881
x1 <- na.omit(x) # NA 제외(방법2)
x1
mean(x1)
shapiro.test(x1)
# 단계5. 정규분포 시각화
hist(x1)
# stats 패키지에서 정규성 검정을 위해서 제공되는 시각화 함수.
qqnorm(x1)
qqline(x1, lty=1, col='blue')
# 단계6. 평균차이 검정
# T-test(T-검정) : 모집단에서 추출한 표본 데이터의 분포형태가 정규분포일 때 수행.
# 1. 양측검정: x1 객체와 기존 모집단의 평균 5.2시간 비교
t.test(x1, mu=5.2) # p-value = 0.0001417
t.test(x1, mu=5.2, alternative = "two.side", conf.level = 0.95)
# 2. 방향성을 갖는 단측가설 검정
t.test(x1, mu=5.2, alternative = "greater", conf.level = 0.95)
# 이항분포 불만족율 기준 비율검정
# 1) 양측검정
binom.test(c(14, 136), p=0.2) # 기존 20% 불만족율 기준 검증 실시
binom.test(c(14, 136), p=0.2, alternative = "two.sided", conf.level = 0.95) # default
# 2. 방향성을 갖는 단측가설 검정
t.test(x1, mu=5.2, alternative = "greater", conf.level = 0.95)
t.test(x1, mu=5.2, alternative = "less", conf.level = 0.95)
# 단계1. 실습데이터 가져오기
data <- read.csv("C:/workspaces/bigdata-master/R/data/two_sample.csv", header = T)
head(data)
str(data)
View(data)
# 단계2. 두 집단 subset 작성 및 데이터 전처리
x <- data$method # 교육방법(1:PT,2:Coding)
y <- data$survey # 만족도(1:만족, 0:불만족)
# 단계3. 집단별 빈도분석
table(x)
table(y)
# 단계4. 두 변수에 대한 교차분석
table(x, y, useNA = "ifany") # 결측치까지 출력
# 단계1. 양측 검정
prop.test(c(110,135), c(150,150)) # PT/Coding 교육 방법에 대한 비율 차이 검정.
# p-value = 0.9998 > 0.05(PT > Coding)
prop.test(c(110,135), c(150,150), alternative = "less", conf.level = 0.95)
# 단계1 : 시계열 자료 생성
input <- c(3180,3000,3200,3100,3300,3200,3400,3550,3200,3400,3300,3700)
# Time Series
tsdata <- ts(input, start = c(2015, 2), frequency = 12)
# 단계2 : 추세선 시각
plot(tsdata, type="l", col="red")
# 단계3 : 자기상관 함수 시각화
acf(na.omit(tsdata), main="자기상관함수", col="red")
# 단계2 : 추세선 시각
plot(tsdata, type="l", col="red")
# 단계4 : 차분 시각화
plot(diff(tsdata, differences=1))
# 단계4 : 차분 시각화
plot(diff(tsdata, differences=2))
# 단계4 : 차분 시각화
plot(diff(tsdata, differences=0.5))
# 단계4 : 차분 시각화
plot(diff(tsdata, differences=1))
# - 이동 평균(Moving Average) : 시계열 자료를 대상으로 일정한 기간의 자료를 평균으로 계산하고, 이동 시킨 추세를 파악하여 추세를 예측하는 분석 기법.
# - 지수 평균 :: 전체 기간 평균 계산 .
# 지수평활 ..?
# 단계1: 시계열  자료 생성
data <- c(45,56,45,43,69,75,58,59,66,64,62,65,
55,49,67,55,71,78,71,65,69,43,70,75,
56,56,65,55,82,85,75,77,77,69,79,89)
length(data) # 36
tsdata <- ts(data, start = c(2016, 1), frequency = 12)
tsdata
library(TTR)
# 단계2 : 평활 관련 패키지 설치
install.packages("TTR")
library(TTR)
# 단계3 : 이동평균법으로 평활 및 시각화
par(mfrow=c(2,2))
plot(tsdata, main="원 시계열 자료") # 시계열 자료 시각화
plot(SMA(tsdata[1:36], n=1), main="1년 단위 이동평균법으로 평활")
plot(SMA(tsdata[1:36], n=2), main="2년 단위 이동평균법으로 평활")
plot(SMA(tsdata[1:36], n=3), main="3년 단위 이동평균법으로 평활")
plot(SMA(tsdata, n=1), main="1년 단위 이동평균법으로 평활")
# 단계3 : 이동평균법으로 평활 및 시각화
par(mfrow=c(2,2))
plot(tsdata, main="원 시계열 자료") # 시계열 자료 시각화
plot(SMA(tsdata, n=1), main="1년 단위 이동평균법으로 평활")
plot(SMA(tsdata, n=2), main="2년 단위 이동평균법으로 평활")
plot(SMA(tsdata, n=3), main="3년 단위 이동평균법으로 평활")
# 단계1: 시계열자료 특성분석
# (1) 데이터 준비
input <- c(3180,3000,3200,3100,3300,3200,3400,3550,3200,3400,3300,3700)
# (2) 시계열 객체 생성(12개월:2015년 2월 ~ 2016년 1월)
tsdata <- ts(input, start = c(2015, 2), frequency = 12)
tsdata
# (3) 추세선 시각화(정상성시계열 vs 비정상성시계열)
plot(tsdata, type="l", col='red')
# 단계2:정상성시계열 변환
par(mfrow=c(1,2))
ts.plot(tsdata)
diff <- diff(tsdata)
plot(diff)
# 단계3: 모형 식별과 추정
install.packages('forecast')
library(forecast)
arima <- auto.arima(tsdata) # 시계열 데이터 이용.
arima
# 단계4: 모형 생성
model <- arima(tsdata, order=c(1,1,0))
model
# 단계5: 모형 진단(모형 타당성 검정)
# (1) 자기상관함수에 의한 모형 진단
tsdiag(model)
# 단계2:정상성시계열 변환
par(mfrow=c(1,1))
# (3) 추세선 시각화(정상성시계열 vs 비정상성시계열)
plot(tsdata, type="l", col='red')
# 단계2:정상성시계열 변환
par(mfrow=c(1,2))
ts.plot(tsdata)
diff <- diff(tsdata)
plot(diff)
# 단계5: 모형 진단(모형 타당성 검정)
# (1) 자기상관함수에 의한 모형 진단
tsdiag(model)
# (2) Box-Ljung에 의한 잔차항 모형 진단
Box.test(model$residuals, lag = 1, type = "Ljung")
# (3) R 에서 HTML 불러오기
library(xml2)
?read_html
library(rvest)
cs_url <- "https://search.naver.com/search.naver?&where=news&query=%EC%B6%94%EC%84%9D&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&cluster_rank=26&refresh_start=0&start="
# url 가져오기
urls <- NULL
for(x in 0:9){
urls <- c(urls, paste(cs_url, x*10+1,sep=''))
}
urls
# (3) R 에서 HTML 불러오기
library(xml2)
html <- read_html(urls[1])
html
library(rvest)
# (4) HTML 에서 필요한 부분 뽑아오기
install.packages("rvest")
library(rvest)
R.version
R.version
cs_url <- "https://search.naver.com/search.naver?&where=news&query=%EC%B6%94%EC%84%9D&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&cluster_rank=26&refresh_start=0&start="
# url 가져오기
urls <- NULL
for(x in 0:9){
urls <- c(urls, paste(cs_url, x*10+1,sep=''))
}
urls
# (3) R 에서 HTML 불러오기
library(xml2)
html <- read_html(urls[1])
html
library(rvest)
htmlnode <- html_nodes(html,'ul.type01 > li > dl > dd > a') # 네이버 뉴스 링크
htmlnode
htmlattr <- html_attr(htmlnode, 'href') # url 주소값 추출
# (5) 뉴스 기사 url 뽑아내기
news_link <- NULL
for(url in urls){
html <- read_html(url)
news_link <- c(news_link, html %>% html_nodes('ul.type01 > li > dl > dd > a') %>%  html_attr('href'))
}
news_link
# (6) 뉴스 본문 / 제목 추출하기
contents <- NULL
title <- NULL
for(link in news_link){
html <- read_html(link)
# 제목
title <- c(title, html %>% html_nodes('#articleTitle') %>% html_text()) # 파싱
# 본문
contents <- c(contents, html %>% html_nodes('._article_body_contents') %>% html_text())
}
contents_df <- as.data.frame(contents)
title_df <- as.data.frame(title)
news <- cbind(title_df,contents_df)
View(news)
# 전처리
# -gsub(패턴,교체문자,자료)
contents_pre <- gsub("[\r\n\t]",' ',contents) # 이스케이프 제거
contents_pre <- gsub("[[:punct:]]",' ',contents_pre) # 문장부호 제거
contents_pre <- gsub("[[:cntrl:]]",' ',contents_pre) # 특수문자 제거
contents_pre <- gsub("\\d+",' ',contents_pre) # 숫자 제거
contents_pre <- gsub("[a-z]+",' ',contents_pre) # 영문 소문자 제거
contents_pre <- gsub("[A-Z]+",' ',contents_pre) # 영문 대문자 제거
contents_pre <- gsub("\\s+",' ',contents_pre) # 2개 이상 공백 교체
head(contents_pre)
library(Sejong)
library(hash)
library(tau)
library(RSQLite)
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jre1.8.0_221')
library(rJava)
library(devtools)
library(KoNLP)
library(wordcloud) ; library(tm)
# 단어 추출을 위한 사용자 정의 함수 정의하기
# (1) 사용자 정의 함수 작성.
#                  - [문자 변환] -> [명사 단어 추출] -> [공백으로 합침]
exNouns <- function(x){
paste(extractNoun(as.character(x)), collapse = " ")
}
# (2) exNouns 함수 이용 단어 추출
facebook_nouns <- sapply(contents_pre, exNouns) # 명사단어 추출
facebook_nouns[1]
# (5) 추출된 단어 대상 전처리
#   1단계 : 추출된 단어 이용 말뭉치 (corpus) 생성
myCorpus <- Corpus(VectorSource(facebook_nouns))
myCorpus
#  2단계 : 데이터 전처리
myCorpusPrepro <- tm_map(myCorpus,removePunctuation) # 문장부호 제거
myCorpusPrepro <- tm_map(myCorpusPrepro,removeNumbers) # 수치 제거
myCorpusPrepro <- tm_map(myCorpusPrepro,tolower) # 소문자 변환
myCorpusPrepro <- tm_map(myCorpusPrepro,removeWords, stopwords('english')) # 불용어 제거 (for, very, and, of, are)
# 전처리 결과 확인
inspect(myCorpusPrepro[1:5])
myCorpusPrepro[1:5]
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro,control = list(wordLengths=c(4,16))) # 텍스트를 숫자로 표현하는 대표직인 방법.
myCorpusPrepro_term # corpus 객체 정보
# matrix 자료구조를 data.frame 자료 구조로 변경 -> 자료구조 확정 짓기.
myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
head(myTerm_df)
dim(myTerm_df) # 696  76
# (7) 단어 출현 빈도수 구하기. - 빈도수가 높은 순서대로 내림차순으로 데이터 정렬
wordResult <- sort(rowSums(myTerm_df), decreasing = T)
wordResult[1:10]
myCorpusPrepro <- tm_map(myCorpus,removePunctuation) # 문장부호 제거
myCorpusPrepro <- tm_map(myCorpusPrepro,removeNumbers) # 수치 제거
myCorpusPrepro <- tm_map(myCorpusPrepro,tolower) # 소문자 변환
mystopwords <- c(stopwords('english'), "사용","하기")
myCorpusPrepro <- tm_map(myCorpusPrepro,removeWords, mystopwords) # 불용어 제거 (for, very, and, of, are)
inspect(myCorpusPrepro[1:5])
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro,control = list(wordLengths=c(4,16))) # 텍스트를 숫자로 표현하는 대표직인 방법.
myCorpusPrepro_term # corpus 객체 정보
myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
wordResult <- sort(rowSums(myTerm_df), decreasing = T)
wordResult[1:20]
# (9) 단어 구름(word cloud) 시각화 -디자인 적용 전
myName <- names(wordResult)
wordcloud(myName, wordResult)
# 단어 구름에 디자인 적용 (빈도수, 색상, 위치 , 회전 등)
# (1) 단어 이름과 빈도수로 data.frame 생성
word.df <- data.frame(word = myName, freq = wordResult)
str(word.df)
# (2) 단어색상과 글꼴 지정
pal <- brewer.pal(12, "Paired")
windowsFonts(malgun=windowsFonts("맑은 고딕"))
wordcloud(word.df$word, word.df$freq, scale = c(5,1), min.freq = 3, random.order = F, rot.per = .1, colors = pal, family="malgun")
set.seed(1234)
wordcloud(words = word.df$word, freq = word.df$freq,
min.freq = 2, max.words = 200,
random.order = F, rot.per = .1,
scale = c(4, 0.3),
colors = pal) # rot.per :: 회전
view(wordResult)
View(wordResult)
write.csv(wordResult,'wordResult.csv')
for(x in 0:100){
urls <- c(urls, paste(cs_url, x*10+1,sep=''))
}
urls
html <- read_html(urls[1])
html
htmlnode <- html_nodes(html,'ul.type01 > li > dl > dd > a') # 네이버 뉴스 링크
htmlnode
htmlattr <- html_attr(htmlnode, 'href') # url 주소값 추출
# (5) 뉴스 기사 url 뽑아내기
news_link <- NULL
for(url in urls){
html <- read_html(url)
news_link <- c(news_link, html %>% html_nodes('ul.type01 > li > dl > dd > a') %>%  html_attr('href'))
}
news_link
# (6) 뉴스 본문 / 제목 추출하기
contents <- NULL
title <- NULL
for(link in news_link){
html <- read_html(link)
# 제목
title <- c(title, html %>% html_nodes('#articleTitle') %>% html_text()) # 파싱
# 본문
contents <- c(contents, html %>% html_nodes('._article_body_contents') %>% html_text())
}
contents_df <- as.data.frame(contents)
title_df <- as.data.frame(title)
news <- cbind(title_df,contents_df)
# 전처리
# -gsub(패턴,교체문자,자료)
contents_pre <- gsub("[\r\n\t]",' ',contents) # 이스케이프 제거
contents_pre <- gsub("[[:punct:]]",' ',contents_pre) # 문장부호 제거
contents_pre <- gsub("[[:cntrl:]]",' ',contents_pre) # 특수문자 제거
contents_pre <- gsub("\\d+",' ',contents_pre) # 숫자 제거
contents_pre <- gsub("[a-z]+",' ',contents_pre) # 영문 소문자 제거
contents_pre <- gsub("[A-Z]+",' ',contents_pre) # 영문 대문자 제거
contents_pre <- gsub("\\s+",' ',contents_pre) # 2개 이상 공백 교체
head(contents_pre)
# 단어 추출을 위한 사용자 정의 함수 정의하기
# (1) 사용자 정의 함수 작성.
#                  - [문자 변환] -> [명사 단어 추출] -> [공백으로 합침]
exNouns <- function(x){
paste(extractNoun(as.character(x)), collapse = " ")
}
# (2) exNouns 함수 이용 단어 추출
facebook_nouns <- sapply(contents_pre, exNouns) # 명사단어 추출
facebook_nouns[1]
# (5) 추출된 단어 대상 전처리
#   1단계 : 추출된 단어 이용 말뭉치 (corpus) 생성
myCorpus <- Corpus(VectorSource(facebook_nouns))
#  2단계 : 데이터 전처리
myCorpusPrepro <- tm_map(myCorpus,removePunctuation) # 문장부호 제거
myCorpusPrepro <- tm_map(myCorpusPrepro,removeNumbers) # 수치 제거
myCorpusPrepro <- tm_map(myCorpusPrepro,tolower) # 소문자 변환
myCorpusPrepro <- tm_map(myCorpusPrepro,removeWords, stopwords('english')) # 불용어 제거 (for, very, and, of, are)
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro,control = list(wordLengths=c(4,16))) # 텍스트를 숫자로 표현하는 대표직인 방법.
myCorpusPrepro_term # corpus 객체 정보
# matrix 자료구조를 data.frame 자료 구조로 변경 -> 자료구조 확정 짓기.
myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
head(myTerm_df)
dim(myTerm_df) # 696  76
# (7) 단어 출현 빈도수 구하기. - 빈도수가 높은 순서대로 내림차순으로 데이터 정렬
wordResult <- sort(rowSums(myTerm_df), decreasing = T)
myCorpusPrepro <- tm_map(myCorpus,removePunctuation) # 문장부호 제거
myCorpusPrepro <- tm_map(myCorpusPrepro,removeNumbers) # 수치 제거
myCorpusPrepro <- tm_map(myCorpusPrepro,tolower) # 소문자 변환
mystopwords <- c(stopwords('english'), "사용","하기")
myCorpusPrepro <- tm_map(myCorpusPrepro,removeWords, mystopwords) # 불용어 제거 (for, very, and, of, are)
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro,control = list(wordLengths=c(4,16))) # 텍스트를 숫자로 표현하는 대표직인 방법.
myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
wordResult <- sort(rowSums(myTerm_df), decreasing = T)
# (9) 단어 구름(word cloud) 시각화 -디자인 적용 전
myName <- names(wordResult)
wordcloud(myName, wordResult)
# 단어 구름에 디자인 적용 (빈도수, 색상, 위치 , 회전 등)
# (1) 단어 이름과 빈도수로 data.frame 생성
word.df <- data.frame(word = myName, freq = wordResult)
wordcloud(words = word.df$word, freq = word.df$freq,
min.freq = 2, max.words = 200,
random.order = F, rot.per = .1,
scale = c(4, 0.3),
colors = pal) # rot.per :: 회전
write.csv(wordResult,'wordResult.csv')
