# (4) 레코드 삽입
query <- 'insert into test_table vlaues("kang","1234","강감찬",35)
# (4) 레코드 삽입
query <- insert into test_table vlaues("kang","1234","강감찬",35)
# (4) 레코드 삽입
query <- "insert into test_table vlaues("kang","1234","강감찬",35)"
# (4) 레코드 삽입
query <- "insert into test_table vlaues('kang','1234','강감찬',35)"
dbSendUpdate(conn,query)
# (1) 모든 레코드 검색
query <- "select * from test_table"
dbGetQuery(conn, query)
# (4) 레코드 삽입
query <- "insert into test_table vlaues('kang','1234','강감찬',35)"
dbSendUpdate(conn,query)
# (4) 레코드 삽입
query <- "insert into test_table values('kang','1234','강감찬',35)"
dbSendUpdate(conn,query)
# (1) 모든 레코드 검색
query <- "select * from test_table"
dbGetQuery(conn, query)
# (1) 패키지 설치 및 준비
install.packages("KoNLP") # 원래 필요한 패키지
library(Sejong)
library(hash)
library(tau)
library(RSQLite)
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jre1.8.0_221')
library(rJava)
library(devtools)
library(KoNLP)
install.packages("https://cran.rstudio.com/bin/windows/contrib/3.4/KoNLP_0.80.1.zip",
repos = NULL)
# Sejong 설치 : KoNLP와 의존성 있는 현재 버전의 한글 사전 패키지 설치
install.packages("Sejong")
install.packages("Sejong")
install.packages(c("hash","tau","RSQLite", "rJava", "devtools"))
source('C:/workspaces/bigdata-master/R/script/chap09_FormalInformal.R', encoding = 'UTF-8', echo=TRUE)
getwd()
library(Sejong)
library(hash)
library(tau)
library(RSQLite)
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jre1.8.0_221')
library(rJava)
library(devtools)
library(KoNLP)
install.packages(c("wordcloud","tm"))
library(wordcloud) ; library(tm)
facebook <- file("C:/workspaces/bigdata-master/R/data/facebook_bigdata.txt", encoding = "UTF-8")
facebook_data <- readLines(facebook)
facebook_data
head(facebook_data) # 줄바꿈 단위로 데이터 생성
str(facebook_data)
# (3) 세종 사전에 신규 단어 추가
userDic <- data.frame(term = c("R 프로그래밍","페이스북","소셜네트워크","얼죽아"), tag='ncn')
# - 신규 단어 사전 추가 함수
buildDictionary(ext_dic = 'sejong', user_dic = userDic)
# (4) 단어 추출을 위한 사용자 정의 함수
# - R 제공 함수 단어 추출 - Sejong 사전에 등록된 신규 단어 테스트
paste(extractNoun("홍길동은 얼죽아를 최애로 생각하는,빅테이터에 최대 관심을 가지고 있으면서, 페이스북이나 소셜네트워크로부터 생성 수집되어진 빅데이터 분석에 아주 많은 관심을 가지고 있어요."), collapse=" ") # extractNoun :: 명사
# (4) 단어 추출을 위한 사용자 정의 함수
# - R 제공 함수 단어 추출 - Sejong 사전에 등록된 신규 단어 테스트
paste(extractNoun("홍길동은 얼죽아를 최애로 생각하는,빅테이터에 최대 관심을 가지고 있으면서, 페이스북이나 소셜네트워크로부터 생성 수집되어진 빅데이터 분석에 아주 많은 관심을 가지고 있어요."), collapse=" ") # extractNoun :: 명사
# (4) 단어 추출을 위한 사용자 정의 함수
# - R 제공 함수 단어 추출 - Sejong 사전에 등록된 신규 단어 테스트
paste(extractNoun("홍길동은 얼죽아를 최애로 생각하는, 빅테이터에 최대 관심을 가지고 있으면서, 페이스북이나 소셜네트워크로부터 생성 수집되어진 빅데이터 분석에 아주 많은 관심을 가지고 있어요."), collapse=" ") # extractNoun :: 명사
# (2) exNouns 함수 이용 단어 추출
facebook_nouns <- sapply(facebook_data, exNouns) # 명사단어 추출출
paste(extractNoun(as.character(x)), collapse = " ")
# (2) exNouns 함수 이용 단어 추출
facebook_nouns <- sapply(facebook_data, exNouns) # 명사단어 추
# 단어 추출을 위한 사용자 정의 함수 정의하기
# (1) 사용자 정의 함수 작성.
#                  - [문자 변환] -> [명사 단어 추출] -> [공백으로 합침]
exNouns <- function(x){
paste(extractNoun(as.character(x)), collapse = " ")
}
# (2) exNouns 함수 이용 단어 추출
facebook_nouns <- sapply(facebook_data, exNouns) # 명사단어 추
facebook_nouns[1]
# (5) 추출된 단어 대상 전처리
#   1단계 : 추출된 단어 이용 말뭉치 (corpus) 생성
myCorpus <- Corpus(VectorSource(facebook_nouns))
myCorpus
myCorpusPrepro <- tm_map(myCorpus,removePunctuation) # 문장부호 제거
myCorpusPrepro <- tm_map(myCorpusPrepro,removeNumbers) # 수치 제거
myCorpusPrepro <- tm_map(myCorpusPrepro,tolower) # 소문자 변환
myCorpusPrepro <- tm_map(myCorpusPrepro,removeWords, stopwords('english')) # 불용어 제거
#(for, very, and, of, are)
# 전처리 결과 확인
inspect(myCorpusPrepro[1:5])
library(Sejong)
library(hash)
library(tau)
library(RSQLite)
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jre1.8.0_221')
library(rJava)
library(devtools)
library(KoNLP)
library(wordcloud) ; library(tm)
# 전처리 결과 확인
inspect(myCorpusPrepro[1:5])
myCorpusPrepro[1:5]
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro,control = list(wordLengths=c(4,16))) # 텍스트를 숫자로 표현하는 대표직인 방법.
myCorpusPrepro_term
# matrix 자료구조를 data.frame 자료 구조로 변경 -> 자료구조 확정 짓기.
myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
myTerm_df
head(myTerm_df)
View(myTerm_df)
dim(myTerm_df
)
# (7) 단어 출현 빈도수 구하기. - 빈도수가 높은 순서대로 내림차순으로 데이터 정렬
wordResult <- sort(rowSums(myTerm_df), decreasing = T)
wordResult
wordResult[1:10]
mystopwords <- c(stopwords('english'), "사용","하기")
myCorpusPrepro <- tm_map(myCorpusPrepro,removeWords, mystopwords) # 불용어 제거 (for, very, and, of, are)
inspect(myCorpusPrepro[1:5])
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro,control = list(wordLengths=c(4,16))) # 텍스트를 숫자로 표현하는 대표직인 방법.
myCorpusPrepro_term # corpus 객체 정보
# matrix 자료구조를 data.frame 자료 구조로 변경 -> 자료구조 확정 짓기.
myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
head(myTerm_df)
# (7) 단어 출현 빈도수 구하기. - 빈도수가 높은 순서대로 내림차순으로 데이터 정렬
wordResult <- sort(rowSums(myTerm_df), decreasing = T)
wordResult[1:10]
wordResult[1:20]
# (9) 단어 구름(word cloud) 시각화 -디자인 적용 전
myName <- names(wordResult)
wordcloud(myName)
wordcloud(myName, wordResult)
# 단어 구름에 디자인 적용 (빈도수, 색상, 위치 , 회전 등)
# (1) 단어 이름과 빈도수로 data.frame 생성
word.df <- data.frame(word = myName, freq = wordResult)
str(word.df)
# (2) 단어색상과 글꼴 지정
pal <- brewer.pal(12, "paired")
windowsFonts(malgun=windowsFonts("맑은 고딕"))
# (3) 단어 구름 시각화
x11() # 별도의 창을 띄우는 함수
wordcloud(word.df$word, word.df$freq, scale = c(5,1), min.freq = 3, random.order = F, rot.per = .1, colors = pal, family="malgun")
# (2) 단어색상과 글꼴 지정
pal <- brewer.pal(12, "paired")
# (2) 단어색상과 글꼴 지정
pal <- brewer.pal(12, "paired")
# (2) 단어색상과 글꼴 지정
pal <- brewer.pal(12, "paired")
?brewer.pal
# (2) 단어색상과 글꼴 지정
pal <- brewer.pal(12, "Paired")
windowsFonts(malgun=windowsFonts("맑은 고딕"))
# (3) 단어 구름 시각화
x11() # 별도의 창을 띄우는 함수
wordcloud(word.df$word, word.df$freq, scale = c(5,1), min.freq = 3, random.order = F, rot.per = .1, colors = pal, family="malgun")
# 1) 실습 파일 가져오기
data <- read.csv("C:/workspaces/bigdata-master/R/data/cleanDescriptive.csv", header = T)
View(data)
# 2) 변수 리코딩
x <- data$level2 # 리코딩 변수 이용(학력수준)
y <- data$pass2  # 리코딩 변수 이용(합격/불합격)
x; y  # 부모학력수준(x) -> 자녀대학진학여부(y)
# 3) 데이터프레임 생성
result <- data.frame(Level=x, Pass=y) # 데이터 프레임 생성 - 데이터 묶음.
# 3) 데이터프레임 생성
result <- data.frame(Level=x, Pass=y) # 데이터 프레임 생성 - 데이터 묶음.
library(gmodels)
# 3) 패키지를 이용한 교차 분할표 생성
CrossTable(x, y) # gmodels
chisq.test(c(4,6,17,16,8,9))
# 5개 제품의 스포츠 음료에 대한 선호도에 차이가 있는지 검정
data <- textConnection(
"스포츠음료종류  관측도수
1    41
2    30
3    51
4    71
5    61
")
x <- read.table(data, header = T)
str(x)
chisq.test(x$관측도수)
data <- read.csv("C:/workspaces/R/data/cleanDescriptive.csv", header = T)
# 독립변수(x) = 설명변수, 종속변수(y) = 반응 변수 생성
x <- data$level2 # 부모의 학력수준
y <- data$pass2  # 자녀의 대학 진학 여부
CrossTable(x, y, chisq = T)
data <- read.csv("C:/workspaces/bigdata-master/R/data/cleanDescriptive.csv", header = T)
# 독립변수(x) = 설명변수, 종속변수(y) = 반응 변수 생성
x <- data$level2 # 부모의 학력수준
y <- data$pass2  # 자녀의 대학 진학 여부
CrossTable(x, y, chisq = T)
# 1. 파일 가져오기
data <- read.csv("C:/workspaces/bigdata-master/R/data/homogenity.csv", header = T)
View(data)
# 전처리 - 결측치/ method와 survey 변수만 서브셋 생성
data <- subset(data, !is.na(survey), c(method, survey))
data
data$method2[data$method == 1] <- "방법1"
data$method2[data$method == 2] <- "방법2"
data$method2[data$method == 3] <- "방법3"
# survey2 필드 추가
data$survey2[data$survey == 1] <- "1. 매우만족"
data$survey2[data$survey == 2] <- "2. 만족"
data$survey2[data$survey == 3] <- "3. 보통"
data$survey2[data$survey == 4] <- "4. 불만족"
data$survey2[data$survey == 5] <- "5. 매우불만족"
table(data$method)
table(data$survey)
# 1. 파일 가져오기
data <- read.csv("C:/workspaces/bigdata-master/R/data/homogenity.csv", header = T)
table(data$method) # 교육방법
# 1  2  3
#50 50 50
table(data$survey) # 만족도
# 1  2  3  4  5
#21 29 37 42 21
# 전처리 - 결측치/ method와 survey 변수만 서브셋 생성
data <- subset(data, !is.na(survey), c(method, survey))
is.na(data$survey)
# 1. 파일 가져오기
data <- read.csv("C:/workspaces/bigdata-master/R/data/homogenity.csv", header = T)
is.na(data$survey)
# 1. 파일 가져오기
data <- read.csv("C:/workspaces/bigdata-master/R/data/homogenity.csv", header = T)
View(data)
# 1. 파일 가져오기
data <- read.csv("C:/workspaces/bigdata-master/R/data/homogenity.csv", header = T)
table(data$method) # 교육방법
# 1  2  3
#50 50 50
table(data$survey) # 만족도
# 1  2  3  4  5
#21 29 37 42 21
# 전처리 - 결측치/ method와 survey 변수만 서브셋 생성
data <- subset(data, !is.na(survey), c(method, survey))
# 3. 교차분할표 작성
table(data$method2, data$survey2) # 교차표 생성 -> table(행, 열)
# method2 필드 추가
data$method2[data$method == 1] <- "방법1"
data$method2[data$method == 2] <- "방법2"
data$method2[data$method == 3] <- "방법3"
# survey2 필드 추가
data$survey2[data$survey == 1] <- "1. 매우만족"
data$survey2[data$survey == 2] <- "2. 만족"
data$survey2[data$survey == 3] <- "3. 보통"
data$survey2[data$survey == 4] <- "4. 불만족"
data$survey2[data$survey == 5] <- "5. 매우불만족"
# 3. 교차분할표 작성
table(data$method2, data$survey2) # 교차표 생성 -> table(행, 열)
# 4. 교차분할표 생성
CrossTable(data$method2, data$survey2, chisq = T)
# 모평균 신뢰수준 95%의 신뢰구간(하한값과 상한값) 구하기
N <- 10000  # 표본크기
X <- 165.1  # 표본평균
S <- 2      # 표본 표준편차
low <- X - 1.96 * S / sqrt(N) # 신뢰구간 하한값
high <- X + 1.96 * S / sqrt(N) # 신뢰구간 상한값
low; high  # 165.0608 ~ 165.1392
# 신뢰구간으로 표본오차 구하기
low - X  # -0.0392 = 신뢰구간 하한값 - 표본 평균
high -X  # 0.0392 = 신뢰구간 상한값 - 표본 평균
# 단계1. 실습데이터 가져오기
getwd()
data <- read.csv("C:/workspaces/bigdata-master/R/data/one_sample.csv", header = T)
head(data)
View(data)
# 단계2. 빈도수와 비율 계산
x <- data$survey
x
summary(x) # 결측치 확인
length(x) # [1] 150
table(x) # 빈도수(0:불만족(14), 1:만족(136))
table(x) # 빈도수(0:불만족(14), 1:만족(136))
# 단계3. 패키지 이용 빈도수와 비율계산
library(prettyR) # freq() 함수 사용
# 단계3. 패키지 이용 빈도수와 비율계산
install.packages("prettyR")
library(prettyR) # freq() 함수 사용
freq(x)
# 1) 양측검정
binom.test(c(136, 14), p=0.8) # 기존 80% 만족율 기준 검증 실시
binom.test(c(136, 14), p=0.8, alternative = "two.sided", conf.level = 0.95)
# 이항분포 불만족율 기준 비율검정
# 1) 양측검정
binom.test(c(14, 136), p=0.2) # 기존 20% 불만족율 기준 검증 실시
# 이항분포 불만족율 기준 비율검정
# 1) 양측검정
binom.test(c(14, 136), p=0.2) # 기존 20% 불만족율 기준 검증 실시
# 2) 방향성을 갖는 단측 가설 검정
binom.test(c(136, 14), p=0.8, alternative = "greater", conf.level = 0.95) # p-value = 0.0003179
binom.test(c(136, 14), p=0.8, alternative = "less", conf.level = 0.95) # p-value = 0.9999
binom.test(c(136, 14), p=0.8, alternative = "two.sided", conf.level = 0.95)
# 단계1. 실습 파일 가져오기
data <- read.csv("C:/workspaces/bigdata-master/R/data/one_sample.csv", header = T)
head(data)
str(data)
x <- data$time
head(x)
# 단계2. 데이터 분포/결측치 제거
summary(x) # NA's -> 41
mean(x)    # [1] NA
# 단계3. 데이터 정제
mean(x, na.rm=T) # NA 제외 평균(방법1) - [1] 5.556881
x1 <- na.omit(x) # NA 제외(방법2)
x1
mean(x1)
shapiro.test(x1)
# 단계5. 정규분포 시각화
hist(x1)
# stats 패키지에서 정규성 검정을 위해서 제공되는 시각화 함수.
qqnorm(x1)
qqline(x1, lty=1, col='blue')
# 단계6. 평균차이 검정
# T-test(T-검정) : 모집단에서 추출한 표본 데이터의 분포형태가 정규분포일 때 수행.
# 1. 양측검정: x1 객체와 기존 모집단의 평균 5.2시간 비교
t.test(x1, mu=5.2) # p-value = 0.0001417
t.test(x1, mu=5.2, alternative = "two.side", conf.level = 0.95)
# 2. 방향성을 갖는 단측가설 검정
t.test(x1, mu=5.2, alternative = "greater", conf.level = 0.95)
# 이항분포 불만족율 기준 비율검정
# 1) 양측검정
binom.test(c(14, 136), p=0.2) # 기존 20% 불만족율 기준 검증 실시
binom.test(c(14, 136), p=0.2, alternative = "two.sided", conf.level = 0.95) # default
# 2. 방향성을 갖는 단측가설 검정
t.test(x1, mu=5.2, alternative = "greater", conf.level = 0.95)
t.test(x1, mu=5.2, alternative = "less", conf.level = 0.95)
# 단계1. 실습데이터 가져오기
data <- read.csv("C:/workspaces/bigdata-master/R/data/two_sample.csv", header = T)
head(data)
str(data)
View(data)
# 단계2. 두 집단 subset 작성 및 데이터 전처리
x <- data$method # 교육방법(1:PT,2:Coding)
y <- data$survey # 만족도(1:만족, 0:불만족)
# 단계3. 집단별 빈도분석
table(x)
table(y)
# 단계4. 두 변수에 대한 교차분석
table(x, y, useNA = "ifany") # 결측치까지 출력
# 단계1. 양측 검정
prop.test(c(110,135), c(150,150)) # PT/Coding 교육 방법에 대한 비율 차이 검정.
# p-value = 0.9998 > 0.05(PT > Coding)
prop.test(c(110,135), c(150,150), alternative = "less", conf.level = 0.95)
library(arules) #read.transactions()함수 제공
# 단계 1 : 연관분석을 위한 패키지 설치
install.packages("arules")
library(arules) #read.transactions()함수 제공
# 단계 2 : 트랜잭션(transaction) 객체 생성
tran<- read.transactions("tran.txt", format="basket", sep=",") # 트랜잭션 객체 생성.
# 단계 2 : 트랜잭션(transaction) 객체 생성
getwd()
setwd("C:/workspaces/bigdata-master/R/data")
tran<- read.transactions("tran.txt", format="basket", sep=",") # 트랜잭션 객체 생성.
tran      # 6개의 트랜잭션과 5개의 항목(상품) 생성.
# 단계 3 : 트랜잭션 데이터 보기
inspect(tran)
# 단계 1 : 트랜잭션 데이터 가져오기
stran2<- read.transactions("single_format.csv", format="single", sep=",", cols=c(1,2), rm.duplicates=T) # sep:각 상품(item)을 구분하는 구분자 지정.
# 단계 2 : 트랜잭션과 상품 수 확인
stran2
inspect(stran2)
# 단계 3 : 요약 통계 제공
summary(stran2) # 248개 트랜잭션에 대한 기술통계 제공
inspect(stran2) # 트랜잭션 확인
# 단계 4 : 규칙(rule) 발견
rule<- apriori(tran, parameter = list(supp=0.3, conf=0.1)) # 16 rule
inspect(rule) # 규칙 보기
rule<- apriori(tran, parameter = list(supp=0.1, conf=0.1)) # 35 rule 낮으면 낮을수록 규칙을 더 많이 찾음
inspect(rule) # 규칙 보기
# [실습]  single 트랜잭션 객체 생성
stran <- read.transactions("demo_single",format="single",cols=c(1,2))
stran
# single : 트랜잭션 구분자(Transaction ID)에 의해서 상품(item)이 대응된 경우이고, basket : 여러 개의 상품(item)으로 구성된 경우(Transaction ID 없이 여러 상품으로만 구성된 경우)
# cols : single인 경우 읽을 컬럼 수 지정(basket은 생략)
inspect(stran)
# [실습] 트랜잭션 관련 정보보기
attributes(Adult)# 트랜잭션의 변수와 범주 보기
# [실습] Adult 데이터 셋 가져오기
data(Adult) # arules에서 제공되는 내장 데이터 로딩
str(Adult) # Formal class 'transactions' , 48842(행)
# [실습] 트랜잭션 관련 정보보기
attributes(Adult)# 트랜잭션의 변수와 범주 보기
data(AdultUCI)
str(AdultUCI) # 'data.frame':	48842 obs. of  2 variables:
# 단계 1 : 지지도를 20%로 높인 경우 1,306개 규칙 발견
ar1<- apriori(Adult, parameter = list(supp=0.2))
# 단계 2 : 지지도 20%, 신뢰도 95% 높인 경우 348개 규칙 발견
ar2<- apriori(Adult, parameter = list(supp=0.2, conf=0.95)) # 신뢰도 높임
# 단계 5 :  지지도 40%, 신뢰도 95% 높인 경우 36 규칙 발견
ar5<- apriori(Adult, parameter = list(supp=0.4, conf=0.95)) # 신뢰도 높임
# 단계 1 : 상위 6개 규칙 보기
inspect(head(ar5))
# 단계 1 : 패키지 설치
install.packages("arulesViz")
library(arulesViz)
# 단계 2 : 연관규칙 시각화
plot(ar3, method='graph', control=list(type='items'))
# 단계 1 : 지지도를 20%로 높인 경우 1,306개 규칙 발견
ar1<- apriori(Adult, parameter = list(supp=0.2))
# 단계 2 : 지지도 20%, 신뢰도 95% 높인 경우 348개 규칙 발견
ar2<- apriori(Adult, parameter = list(supp=0.2, conf=0.95)) # 신뢰도 높임
# 단계 3 : 지지도 30%, 신뢰도 95% 높인 경우 124개 규칙 발견
ar3<- apriori(Adult, parameter = list(supp=0.3, conf=0.95)) # 신뢰도 높임
# 단계 4 :  지지도 35%, 신뢰도 95% 높인 경우 67 규칙 발견
ar4<- apriori(Adult, parameter = list(supp=0.35, conf=0.95)) # 신뢰도 높임
# 단계 5 :  지지도 40%, 신뢰도 95% 높인 경우 36 규칙 발견
ar5<- apriori(Adult, parameter = list(supp=0.4, conf=0.95)) # 신뢰도 높임
# 단계 2 : 연관규칙 시각화
plot(ar3, method='graph', control=list(type='items'))
data("Groceries")  # 식료품점 데이터 로딩
str(Groceries) # Formal class 'transactions' [package "arules"] with 4 slots
Groceries
# 단계 2 : data.frame으로 형 변환
Groceries.df<- as(Groceries, "data.frame")
head(Groceries.df)
# 단계 3 : 지지도 0.001, 신뢰도 0.8 적용 규칙 발견(410 rule(s))
rules <- apriori(Groceries, parameter=list(supp=0.001, conf=0.8))
inspect(rules)
# 단계 4 : 규칙을 구성하는 왼쪽(LHS) -> 오른쪽(RHS)의 item 빈도수 보기
library(arulesViz)
plot(rules, method="grouped")
# 단계1: 데이터 셋 가져오기
data("AirPassengers") # 12년(1949~1960년)간 매월 항공기 탑승 승객 수를 기록한 시계열 자료.
str(AirPassengers) # Time-Series [1:144] from 1949 to 1961:
par(mfrow=c(1,2))
ts.plot(AirPassengers)
log <- diff(log(AirPassengers)) # 로그+차분 수행
plot(log)
# 단계1: WWWusage 데이터 셋 가져오기 - R에서 기본 제공 데이터 셋으로 인터넷 사용 시간을 분 단위로 측정한 100개 vector로 구성된 시계열 자료.
data("WWWusage")
str(WWWusage) # Time-Series [1:100] from 1 to 100:
WWWusage
ts.plot(WWWusage, type="l", col="red")
par(mfrow=c(1,2))
ts.plot(AirPassengers)
log <- diff(AirPassengers)
plot(log) # 차분 정상화
par(mfrow=c(1,2))
log <- diff(AirPassengers)
plot(log) # 차분 정상화
log <- diff(log(AirPassengers)) # 로그+차분 수행 : 분산 정상화 (차분함수 :: diff)
plot(log) # 분산 정상화
ts.plot(WWWusage, type="l", col="red")
par(mfrow=c(1,1)
par(mfrow=c(1,1))
par(mfrow=c(1,1))
ts.plot(WWWusage, type="l", col="red")
# 단계1 : 데이터 셋 가져오기
data("EuStockMarkets") # 유럽(1991~1998년)의 주요 주식의 주가지수 일일 마감 가격.
head(EuStockMarkets) # DAX(독일) SMI(스위스) CAC(프랑스) FTSE(영국)
str(EuStockMarkets) # Time-Series [1:1860, 1:4]
# 단계2 : 데이터프레임으로 변환
EuStock <- data.frame(EuStockMarkets)
EuStock
head(EuStock)
plot(EuStock$DAX[1:1000], type = "l", col="red") # 선 그래프 시각화
# 단계4 : 다중 시계열 데이터 추세선
plot.ts(cbind(EuStock$DAX[1:1000], EuStock$SMI[1:1000]), main="주가지수 추세선")
data <- c(45,56,45,43,69,75,58,59,66,64,62,65,
55,49,67,55,71,78,71,65,69,43,70,75,
56,56,65,55,82,85,75,77,77,69,79,89)
length(data) # 36
data <- c(45,56,45,43,69,75,58,59,66,64,62,65, # 12 2016
55,49,67,55,71,78,71,65,69,43,70,75, # 12 2017
56,56,65,55,82,85,75,77,77,69,79,89) # 12 2018
length(data) # 36
# 단계2 : 시계열자료 생성 : 시계열 자료 형식으로 객체 생성
tsdata <- ts(data, start = c(2016, 1), frequency = 12)
tsdata # 2016~2018
ts.plot(tsdata) # plot(tsdata)와 동일함.
# 단계4 : 시계열 분해- stl():계절요소, 추세, 잔차 모두 제공.
plot(stl(tsdata, "periodic")) # periodic : 주기
# 단계5 : 시계열 분해와 변동 요인 제거
m <- decompose(tsdata) # decompose()함수 이용 시계열 분해
attributes(m) # 변수 보기
plot(m) # 추세요인, 계절요인, 불규칙 요인이 포함된 그래프.
plot(tsdata - m$seasonal) # 계절요인을 제거한 그래프.
# 단계3 : 추세선 확인
par(mfrow=c(2,2))
plot(tsdata - m$seasonal) # 계절요인을 제거한 그래프.
# 단계6 : 추세요인과 불규칙요인 제거
plot(tsdata - m$trend) # 추세요인 제거 그래프
plot(tsdata - m$seasonal - m$trend) # 불규칙 요인만 출력.
plot(tsdata - m$seasonal) # 계절요인을 제거한 그래프.
# 단계6 : 추세요인과 불규칙요인 제거
plot(tsdata - m$trend) # 추세요인 제거 그래프
plot(tsdata - m$seasonal - m$trend) # 불규칙 요인만 출력.
# 단계3 : 추세선 확인
par(mfrow=c(3,1))
plot(tsdata - m$seasonal) # 계절요인을 제거한 그래프.
# 단계6 : 추세요인과 불규칙요인 제거
plot(tsdata - m$trend) # 추세요인 제거 그래프
plot(tsdata - m$seasonal - m$trend) # 불규칙 요인만 출력.
# 단계3 : 추세선 확인
par(mfrow=c(2,1))
plot(tsdata - m$seasonal - m$trend) # 불규칙 요인만 출력.
plot(m$random)
ts.plot(tsdata) # plot(tsdata)와 동일함.
# 단계3 : 추세선 확인
par(mfrow=c(2,1))
ts.plot(tsdata) # plot(tsdata)와 동일함.
plot(m$x)
# 단계1 : 시계열자료 생성
input <- c(3180,3000,3200,3100,3300,3200,3400,3550,3200,3400,3300,3700)
tsdata <- ts(input, start = c(2015, 2), frequency = 12) # Time Series
tsdata
# 단계2 : 자기상관함수 시각화
acf(na.omit(tsdata), main="자기상관함수", col="red")
# 단계3 : 부분자기상관함수 시각화
pacf(na.omit(tsdata), main="부분자기상관함수", col="red")
